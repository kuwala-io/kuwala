{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# POI Category by Population\n",
    "\n",
    "With this notebook you can correlate any value associated with a geo-reference with the Google popularity score. You\n",
    "can upload your own file as a CSV. The only thing that is necessary to make it work is to have columns for latitude and\n",
    "longitude and column headers.\n",
    "\n",
    "The value columns can be specific to your use case, e.g., scooter bookings, sales in shops or crimes. The popularity\n",
    "score is aggregated on a week. So ideally, the value columns that you want to correlate are aggregated on a weekly\n",
    "timeframe as well.\n",
    "\n",
    "As an example we are using an open data set from Uber that gives us the traversals of rides through specific hexagons.\n",
    "You can find the raw data on [their open data platform]('https://movement.uber.com/?lang=en-US). We preprocessed the raw\n",
    "data so that the traversals are already aggregated per week.\n",
    "\n",
    "## 1. Set Parameters\n",
    "\n",
    "1. Set the file path to your CSV and the delimiter. Simply place your file under `kuwala/resources` from within the\n",
    "Jupyter environment or under `kuwala/common/jupyter/resources` from the repository root on your local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "file_path = '../resources/lisbon_uber_traversals.csv'\n",
    "delimiter = ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "2. Set the H3 resolution to aggregate the results on.\n",
    "\n",
    "    To see the average size of a hexagon at a given resolution go to the\n",
    "    [official H3 documentation](https://h3geo.org/docs/core-library/restable). The currently set resolution 8 has on\n",
    "    average an edge length of 0.46 km which can be freely interpreted as a radius."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "resolution = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3. Set the column names for the coordinates and the columns of the file you want to correlate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lat_column = 'latitude'\n",
    "lng_column = 'longitude'\n",
    "value_columns = ['weekly_traversals']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "4. You can provide polygon coordinates as a GeoJSON-conform array to select a subregion. Otherwise, data form the entire\n",
    "database will be analyzed. (The default coordinates are a rough representation of Lisbon, Portugal.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "polygon_coords = '[[[-9.092559814453125,38.794500078219826],[-9.164314270019531,38.793429729760994],[-9.217529296875,38.76666579487878],[-9.216842651367188,38.68792166352608],[-9.12139892578125,38.70399894245585],[-9.0911865234375,38.74551518488265],[-9.092559814453125,38.794500078219826]]]'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load dataframes\n",
    "\n",
    "#### Create a Spark session that is used to load your file and connect to the Neo4j instance ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/usr/local/spark-3.1.2-bin-hadoop3.2/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/jovyan/.ivy2/cache\n",
      "The jars for the packages stored in: /home/jovyan/.ivy2/jars\n",
      "neo4j-contrib#neo4j-connector-apache-spark_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-6327a073-4cf1-4726-8ff1-5ac47b8f630a;1.0\n",
      "\tconfs: [default]\n",
      "\tfound neo4j-contrib#neo4j-connector-apache-spark_2.12;4.0.1_for_spark_3 in spark-packages\n",
      ":: resolution report :: resolve 334ms :: artifacts dl 5ms\n",
      "\t:: modules in use:\n",
      "\tneo4j-contrib#neo4j-connector-apache-spark_2.12;4.0.1_for_spark_3 from spark-packages in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-6327a073-4cf1-4726-8ff1-5ac47b8f630a\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 1 already retrieved (0kB/9ms)\n",
      "21/09/23 16:32:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from kuwala.modules.common import get_spark_session\n",
    "\n",
    "sp = get_spark_session(memory_in_gb=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Load the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+\n",
      "|       h3_index|sum(weekly_traversals)|\n",
      "+---------------+----------------------+\n",
      "|89393375e7bffff|               10297.0|\n",
      "|89393367677ffff|              162622.0|\n",
      "|89393362b93ffff|               64161.0|\n",
      "|8939336057bffff|              932284.0|\n",
      "|893933759abffff|               24961.0|\n",
      "|8939336764fffff|              399611.0|\n",
      "|8939336742fffff|                2264.0|\n",
      "|89393375e6bffff|               41127.0|\n",
      "|89393367557ffff|               48067.0|\n",
      "|8939337533bffff|                  90.0|\n",
      "+---------------+----------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from geojson import Polygon\n",
    "from kuwala.modules.common import add_h3_index_column, polyfill_polygon\n",
    "\n",
    "df_file = sp.read.option('delimiter', delimiter).csv(file_path, header=True)\n",
    "df_file = add_h3_index_column(df=df_file, lat_column=lat_column, lng_column=lng_column, resolution=resolution)\n",
    "\n",
    "if polygon_coords:\n",
    "    polygon_coords_json = json.loads(polygon_coords)\n",
    "    polygon = Polygon(polygon_coords_json)\n",
    "    h3_index_in_polygon = list(polyfill_polygon(polygon=polygon, resolution=resolution))\n",
    "    df_file = df_file.filter(df_file.h3_index.isin(h3_index_in_polygon))\n",
    "\n",
    "aggregations = { x: 'sum' for x in value_columns}\n",
    "df_file = df_file.select('h3_index', *value_columns).groupBy('h3_index').agg(aggregations)\n",
    "\n",
    "df_file.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Get weekly popularity per hexagon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|       h3_index|weekly_popularity|\n",
      "+---------------+-----------------+\n",
      "|89393360193ffff|             5751|\n",
      "|89393360197ffff|             8050|\n",
      "|893933601b3ffff|             1636|\n",
      "|89393360503ffff|             6298|\n",
      "|8939336050bffff|             2110|\n",
      "|89393360517ffff|             5774|\n",
      "|8939336051bffff|             9491|\n",
      "|89393360523ffff|             4092|\n",
      "|8939336052bffff|            25906|\n",
      "|8939336052fffff|             3618|\n",
      "+---------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from kuwala.modules.popularity_controller import get_weekly_popularity_in_h3\n",
    "\n",
    "popularity = get_weekly_popularity_in_h3(sp=sp, resolution=resolution, polygon_coords=polygon_coords)\n",
    "\n",
    "popularity.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Join dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------------------+-----------------+\n",
      "|       h3_index|sum(weekly_traversals)|weekly_popularity|\n",
      "+---------------+----------------------+-----------------+\n",
      "|89393362b93ffff|               64161.0|            45392|\n",
      "|89393367677ffff|              162622.0|            11512|\n",
      "|89393375e7bffff|               10297.0|            11538|\n",
      "|8939336057bffff|              932284.0|             3104|\n",
      "|8939336742fffff|                2264.0|             2433|\n",
      "|8939336764fffff|              399611.0|            41947|\n",
      "|893933759abffff|               24961.0|                0|\n",
      "|89393362c8bffff|                7609.0|            16094|\n",
      "|89393367557ffff|               48067.0|                0|\n",
      "|8939337533bffff|                  90.0|                0|\n",
      "+---------------+----------------------+-----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "popularity = popularity.withColumnRenamed('h3_index', 'join_h3_index')\n",
    "result = df_file \\\n",
    "    .join(popularity, df_file.h3_index == popularity.join_h3_index, 'left') \\\n",
    "    .drop('join_h3_index') \\\n",
    "    .fillna(0, subset=['weekly_popularity'])\n",
    "\n",
    "result.show(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Results\n",
    "\n",
    "#### Pandas Profiling Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ce5d67aa0d1451390ac514a5d5472ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Summarize dataset:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b533abc6cb4abaa7535493a2f08737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generate report structure:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Render widgets:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69eb79bded3645d586cc7626ec15f3b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(Tab(children=(GridBox(children=(VBox(children=(GridspecLayout(children=(HTML(valuâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "result_pd = result.toPandas()\n",
    "profile = ProfileReport(result_pd, title=\"Pandas Profiling Report\", explorative=True)\n",
    "\n",
    "profile.to_widgets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
